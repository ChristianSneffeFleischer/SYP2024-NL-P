{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read NER data\n",
    "def read_universal_NER(file_path):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as infile:\n",
    "        # Split into lines\n",
    "        lines = infile.readlines()\n",
    "\n",
    "        # Define lists to store data \n",
    "        sentences = []\n",
    "        labels = []\n",
    "        current_sentence = []\n",
    "        current_labels = []\n",
    "\n",
    "        # Iterate over lines\n",
    "        for line in lines:\n",
    "\n",
    "            line = line.strip() # Remove whitespace\n",
    "            if not line: # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # Check if line starts with sentence ID\n",
    "            if line.startswith('# sent_id'):\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "\n",
    "            # Check for token lines\n",
    "            elif not line.startswith(\"#\"):\n",
    "                parts = line.strip().split('\\t')\n",
    "                current_sentence.append(parts[1])\n",
    "                current_labels.append(parts[2])\n",
    "\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "\n",
    "    # Flatten lists\n",
    "    #sentences = sum(sentences, [])\n",
    "    #labels = sum(labels, [])\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Date\n",
    "test_sents, test_labels = read_universal_NER('en_ewt-ud-test.iob2')\n",
    "train_sents, train_labels = read_universal_NER('en_ewt-ud-train.iob2')\n",
    "\n",
    "# training and validation\n",
    "train_data = list(zip(train_sents, train_labels))\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Unzip the training and validation data\n",
    "train_sents, train_labels = zip(*train_data)\n",
    "validation_sents, validation_labels = zip(*validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Mappng\n",
    "label2id = {\n",
    " 'O': 0,\n",
    " 'B-LOC': 1,\n",
    " 'I-LOC': 2,\n",
    " 'B-PER': 3,\n",
    " 'I-PER': 4,\n",
    " 'B-ORG': 5,\n",
    " 'I-ORG': 6,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_map2id(labels, mapping):\n",
    "    label_ids = []\n",
    "    for sent in labels:\n",
    "        label_id = [mapping[label] for label in sent]\n",
    "        label_ids.append(label_id)\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_ids = labels_map2id(train_labels, label2id)\n",
    "validation_label_ids = labels_map2id(validation_labels, label2id)\n",
    "test_label_ids = labels_map2id(test_labels, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_sents, \"ner_tags\": train_label_ids})\n",
    "validation_dataset = Dataset.from_dict({\"tokens\": validation_sents, \"ner_tags\": validation_label_ids})\n",
    "test_dataset = Dataset.from_dict({\"tokens\": test_sents, \"ner_tags\": test_label_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 10034\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2509\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2077\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "# Print to verify\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/myenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words = True, truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55bbdb77aa460f9ab97ef610ec809c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94361f9e9b7a452e890200eb5deac5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4541c342fb7a427da22a934d6f7d6a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/myenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d89f5d167a446c99d2a3b5a57dfcb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1563, 'grad_norm': 0.41864439845085144, 'learning_rate': 1.7343957503320053e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0701, 'grad_norm': 6.179903984069824, 'learning_rate': 1.4687915006640108e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c97f1d92c6b4879a7a8ac6c4b02571d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06762628257274628, 'eval_precision': 0.7664630006788866, 'eval_recall': 0.7928370786516854, 'eval_f1': 0.7794269934414911, 'eval_accuracy': 0.9797981865444051, 'eval_runtime': 62.9269, 'eval_samples_per_second': 39.872, 'eval_steps_per_second': 4.99, 'epoch': 1.0}\n",
      "{'loss': 0.056, 'grad_norm': 3.7462689876556396, 'learning_rate': 1.2031872509960161e-05, 'epoch': 1.2}\n",
      "{'loss': 0.036, 'grad_norm': 1.8037104606628418, 'learning_rate': 9.375830013280214e-06, 'epoch': 1.59}\n",
      "{'loss': 0.0372, 'grad_norm': 1.9983482360839844, 'learning_rate': 6.719787516600266e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d20f458a1a4454b00e3d19d572d70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06432849168777466, 'eval_precision': 0.7693290734824281, 'eval_recall': 0.8455056179775281, 'eval_f1': 0.8056206088992974, 'eval_accuracy': 0.9826841598952044, 'eval_runtime': 17.1709, 'eval_samples_per_second': 146.12, 'eval_steps_per_second': 18.287, 'epoch': 2.0}\n",
      "{'loss': 0.0167, 'grad_norm': 0.027124391868710518, 'learning_rate': 4.06374501992032e-06, 'epoch': 2.39}\n",
      "{'loss': 0.0205, 'grad_norm': 0.7067036628723145, 'learning_rate': 1.407702523240372e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3a393c40f44fdabeaadda5aff40d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06831907480955124, 'eval_precision': 0.7924403183023873, 'eval_recall': 0.839185393258427, 'eval_f1': 0.8151432469304228, 'eval_accuracy': 0.9840759768303416, 'eval_runtime': 14.4109, 'eval_samples_per_second': 174.105, 'eval_steps_per_second': 21.789, 'epoch': 3.0}\n",
      "{'train_runtime': 1830.307, 'train_samples_per_second': 16.446, 'train_steps_per_second': 2.057, 'train_loss': 0.05359043194794876, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3765, training_loss=0.05359043194794876, metrics={'train_runtime': 1830.307, 'train_samples_per_second': 16.446, 'train_steps_per_second': 2.057, 'total_flos': 346005790735176.0, 'train_loss': 0.05359043194794876, 'epoch': 3.0})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ChristianSneffeFleischer/bert-finetuned-ner/commit/2f9e9b8bc212518bf41e5ebd82edbcd8c71703da', commit_message='Training complete', commit_description='', oid='2f9e9b8bc212518bf41e5ebd82edbcd8c71703da', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80219facb0594927b52a628575b1ddd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06831907480955124, 'eval_precision': 0.7924403183023873, 'eval_recall': 0.839185393258427, 'eval_f1': 0.8151432469304228, 'eval_accuracy': 0.9840759768303416, 'eval_runtime': 25.5896, 'eval_samples_per_second': 98.048, 'eval_steps_per_second': 12.271, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a600bd953119414e88c593ceeb0743a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions, labels, _ = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_iob2_predictions_shifted(test_sents, predictions, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for i, (sent, prediction) in enumerate(zip(test_sents, predictions)):\n",
    "            outfile.write(f\"# sent_id = test-{i+1:04d}\\n\")\n",
    "            outfile.write(f\"# text = {' '.join(sent)}\\n\")\n",
    "            # Shift predictions up by one position\n",
    "            shifted_predictions = prediction[1:] + [\"O\"]\n",
    "            for j, (word, label) in enumerate(zip(sent, shifted_predictions)):\n",
    "                outfile.write(f\"{j+1}\\t{word}\\t{label}\\t-\\t-\\n\")\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to label indices\n",
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label indices to label names\n",
    "true_predictions = [\n",
    "    [id2label[p] for p in prediction]\n",
    "    for prediction in predictions\n",
    "]\n",
    "\n",
    "# Write the shifted predictions to a file\n",
    "write_iob2_predictions_shifted(test_sents, true_predictions, 'test_predictions2.iob2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/myenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.99479514,\n",
       "  'word': 'Argentina',\n",
       "  'start': 3,\n",
       "  'end': 12}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "model_checkpoint = \"ChristianSneffeFleischer/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"In Argentina , beef is revered , respected , and praised .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
