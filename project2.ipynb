{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project NLP and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project proposal presentation\n",
    "\n",
    "In the presentation, you have 5 minutes to present your research proposal. During the presentation, you should explain:\n",
    "* What was your baseline model (architecture, design decisions etc.)\n",
    "* What is the topic of your project, what is the current state of this topic/task/setup\n",
    "* What is the new part of your project\n",
    "* What is the research question of your project\n",
    "\n",
    "We have proposed a number of topics in the slides which can be found on LearnIt, you can either pick one of these or come up with your own. If you pick your own, we suggest to get a pre-approval with Rob van der Goot.\n",
    "\n",
    "**Deadline for uploading slides: 12-03 on LearnIt (14:00)**  (pdf only, they will be put into one long pdf for a smooth presentation)\n",
    "\n",
    "**Presentations: 13-03 from 08:00-12:00**, we will split the class in half for the lecture hours (08:00-10:00) and the lab hours (10:00-12:00)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Baseline\n",
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the [Universal NER project](http://www.universalner.org/), which we provide with this notebook for convenience. You can use the train data (`en_ewt-ud-train.iob2`) and dev data(`en_ewt-ud-dev.iob2`) to build your baseline, then upload your prediction on the test data (`en_ewt-ud-test.iob2`).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and dev files, so that the `span_f1.py` script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point.\n",
    "\n",
    "**Deadline: 20-03 on LearnIt (11:59)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import myutils\n",
    "from typing import List\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Function to Read the Universel NER Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read NER data\n",
    "\n",
    "def read_universal_NER(file_path):\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as infile:\n",
    "        # Split into lines\n",
    "        lines = infile.readlines()\n",
    "\n",
    "        # Define lists to store data \n",
    "        sentences = []\n",
    "        labels = []\n",
    "        current_sentence = []\n",
    "        current_labels = []\n",
    "\n",
    "        # Iterate over lines\n",
    "        for line in lines:\n",
    "\n",
    "            line = line.strip() # Remove whitespace\n",
    "            if not line: # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # Check if line starts with sentence ID\n",
    "            if line.startswith('# sent_id'):\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "\n",
    "            # Check for token lines\n",
    "            elif not line.startswith(\"#\"):\n",
    "                parts = line.strip().split('\\t')\n",
    "                current_sentence.append(parts[1])\n",
    "                current_labels.append(parts[2])\n",
    "\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "\n",
    "    # Flatten lists\n",
    "    sentences = sum(sentences, [])\n",
    "    labels = sum(labels, [])\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the BERT Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXISTING BERT MODEL FROM EX5\n",
    "\n",
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "    \n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        predictions : list\n",
    "            list of predicted labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Store correct and total predictions\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Create empty list to store predictions\n",
    "        predictions = []\n",
    "\n",
    "        # Testing\n",
    "        printed = False\n",
    "        \n",
    "        # Iterate over test data\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "\n",
    "            # Perform forward pass\n",
    "            output_scores = self.forward(sents)\n",
    "\n",
    "            # # Testing\n",
    "            # if not printed:\n",
    "            #     print(f'output_scores shape: {output_scores.shape}')\n",
    "            #     print(f'output_scores (first 10 instances of first batch):\\n {output_scores[:10]}')\n",
    "            #     printed = True\n",
    "\n",
    "            # Get prediction labels\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "\n",
    "            # Convert predictions back to tags and append to list\n",
    "            predictions.append(pred_labels)\n",
    "\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    correct+= 1\n",
    "\n",
    "        correct_freq = correct / total\n",
    "\n",
    "        return correct_freq, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a Function to Train the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ClassModel(train_file_path: str, \n",
    "                     dev_file_path: str,\n",
    "                     MLM: str,\n",
    "                     UNK: str,\n",
    "                     lr: float,\n",
    "                     batch_size: int,\n",
    "                     device: str,\n",
    "                     n_epochs: int,\n",
    "                     max_train_sents = None,\n",
    "                     return_model = False,\n",
    "                     return_mappings = False\n",
    "                     ):\n",
    "\n",
    "    # Read data\n",
    "    print('reading data...')\n",
    "    train_sents, train_labels = read_universal_NER(train_file_path)\n",
    "    dev_sents, dev_labels = read_universal_NER(dev_file_path)\n",
    "    \n",
    "    # Slice train data if max_train_sents is passed\n",
    "    if max_train_sents is not None:\n",
    "        train_sents = train_sents[:max_train_sents]\n",
    "        train_labels = train_labels[:max_train_sents]\n",
    "\n",
    "    id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "    n_labels = len(id2label)\n",
    "\n",
    "    # Transform labels to numerical\n",
    "    train_labels = [label2id.get(label, label2id[UNK]) for label in train_labels]\n",
    "    dev_labels = [label2id.get(label, label2id[UNK]) for label in dev_labels]\n",
    "    \n",
    "    # Tokenize\n",
    "    print('tokenizing...')\n",
    "    tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "    train_tokked = myutils.tok(train_sents, tokzr)\n",
    "    dev_tokked = myutils.tok(dev_sents, tokzr)\n",
    "    PAD = tokzr.pad_token_id\n",
    "    \n",
    "    # Convert to batches\n",
    "    print('converting to batches...')\n",
    "    train_sents_batched, train_labels_batched = myutils.to_batch(train_tokked, train_labels, batch_size, PAD, device)\n",
    "    dev_sents_batched, dev_labels_batched = myutils.to_batch(dev_tokked, dev_labels, batch_size, PAD, device)\n",
    "    \n",
    "    # Create instance of model\n",
    "    print('initializing model...')\n",
    "    model = ClassModel(n_labels, MLM)\n",
    "    model.to(device) # Move to device\n",
    "\n",
    "    # Define optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index = 0, reduction = 'sum')\n",
    "    \n",
    "    print('training...')\n",
    "    for epoch in range(n_epochs):\n",
    "        print('=====================')\n",
    "        print(f'starting epoch {epoch + 1}/{n_epochs}')\n",
    "\n",
    "        # Set model to training model\n",
    "        model.train() \n",
    "    \n",
    "        # Keep total epoch loss\n",
    "        loss = .0\n",
    "\n",
    "        # Loop over batches\n",
    "        for batch_idx in range(0, len(train_sents_batched)):\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'running for batch {batch_idx}/{len(train_sents_batched)}')\n",
    "\n",
    "            # Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            output_scores = model.forward(train_sents_batched[batch_idx])\n",
    "\n",
    "            # Compute loss for batch and add to total loss\n",
    "            batch_loss = criterion(output_scores, train_labels_batched[batch_idx])\n",
    "            loss += batch_loss.item()\n",
    "    \n",
    "            # Perform backward pass\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Compute dev accuracy\n",
    "        dev_score, dev_preds = model.run_eval(dev_sents_batched, dev_labels_batched)\n",
    "\n",
    "        # Print statistics\n",
    "        print(f'Training Loss: {loss:.4f}')\n",
    "        print(f'Dev Accuracy: {dev_score:.4f}')\n",
    "\n",
    "        # # Testing\n",
    "        # dev_preds_unique = torch.unique(torch.cat(dev_preds))\n",
    "        # print('Dev Predictions:')\n",
    "        # print(dev_preds_unique.tolist())\n",
    "\n",
    "    # Return logic, not very nice code but it works for now\n",
    "    returns = []\n",
    "    if return_model:\n",
    "        returns.append(model)\n",
    "    if return_mappings:\n",
    "        returns.append(label2id)\n",
    "        returns.append(id2label)\n",
    "    if returns:\n",
    "        return tuple(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "tokenizing...\n",
      "converting to batches...\n",
      "initializing model...\n",
      "training...\n",
      "=====================\n",
      "starting epoch 1/5\n",
      "running for batch 0/6393\n",
      "running for batch 100/6393\n",
      "running for batch 200/6393\n",
      "running for batch 300/6393\n",
      "running for batch 400/6393\n",
      "running for batch 500/6393\n",
      "running for batch 600/6393\n",
      "running for batch 700/6393\n",
      "running for batch 800/6393\n",
      "running for batch 900/6393\n",
      "running for batch 1000/6393\n",
      "running for batch 1100/6393\n",
      "running for batch 1200/6393\n",
      "running for batch 1300/6393\n",
      "running for batch 1400/6393\n",
      "running for batch 1500/6393\n",
      "running for batch 1600/6393\n",
      "running for batch 1700/6393\n",
      "running for batch 1800/6393\n",
      "running for batch 1900/6393\n",
      "running for batch 2000/6393\n",
      "running for batch 2100/6393\n",
      "running for batch 2200/6393\n",
      "running for batch 2300/6393\n",
      "running for batch 2400/6393\n",
      "running for batch 2500/6393\n",
      "running for batch 2600/6393\n",
      "running for batch 2700/6393\n",
      "running for batch 2800/6393\n",
      "running for batch 2900/6393\n",
      "running for batch 3000/6393\n",
      "running for batch 3100/6393\n",
      "running for batch 3200/6393\n",
      "running for batch 3300/6393\n",
      "running for batch 3400/6393\n",
      "running for batch 3500/6393\n",
      "running for batch 3600/6393\n",
      "running for batch 3700/6393\n",
      "running for batch 3800/6393\n",
      "running for batch 3900/6393\n",
      "running for batch 4000/6393\n",
      "running for batch 4100/6393\n",
      "running for batch 4200/6393\n",
      "running for batch 4300/6393\n",
      "running for batch 4400/6393\n",
      "running for batch 4500/6393\n",
      "running for batch 4600/6393\n",
      "running for batch 4700/6393\n",
      "running for batch 4800/6393\n",
      "running for batch 4900/6393\n",
      "running for batch 5000/6393\n",
      "running for batch 5100/6393\n",
      "running for batch 5200/6393\n",
      "running for batch 5300/6393\n",
      "running for batch 5400/6393\n",
      "running for batch 5500/6393\n",
      "running for batch 5600/6393\n",
      "running for batch 5700/6393\n",
      "running for batch 5800/6393\n",
      "running for batch 5900/6393\n",
      "running for batch 6000/6393\n",
      "running for batch 6100/6393\n",
      "running for batch 6200/6393\n",
      "running for batch 6300/6393\n",
      "Training Loss: 38492.3379\n",
      "Dev Accuracy: 0.9404\n",
      "=====================\n",
      "starting epoch 2/5\n",
      "running for batch 0/6393\n",
      "running for batch 100/6393\n",
      "running for batch 200/6393\n",
      "running for batch 300/6393\n",
      "running for batch 400/6393\n",
      "running for batch 500/6393\n",
      "running for batch 600/6393\n",
      "running for batch 700/6393\n",
      "running for batch 800/6393\n",
      "running for batch 900/6393\n",
      "running for batch 1000/6393\n",
      "running for batch 1100/6393\n",
      "running for batch 1200/6393\n",
      "running for batch 1300/6393\n",
      "running for batch 1400/6393\n",
      "running for batch 1500/6393\n",
      "running for batch 1600/6393\n",
      "running for batch 1700/6393\n",
      "running for batch 1800/6393\n",
      "running for batch 1900/6393\n",
      "running for batch 2000/6393\n",
      "running for batch 2100/6393\n",
      "running for batch 2200/6393\n",
      "running for batch 2300/6393\n",
      "running for batch 2400/6393\n",
      "running for batch 2500/6393\n",
      "running for batch 2600/6393\n",
      "running for batch 2700/6393\n",
      "running for batch 2800/6393\n",
      "running for batch 2900/6393\n",
      "running for batch 3000/6393\n",
      "running for batch 3100/6393\n",
      "running for batch 3200/6393\n",
      "running for batch 3300/6393\n",
      "running for batch 3400/6393\n",
      "running for batch 3500/6393\n",
      "running for batch 3600/6393\n",
      "running for batch 3700/6393\n",
      "running for batch 3800/6393\n",
      "running for batch 3900/6393\n",
      "running for batch 4000/6393\n",
      "running for batch 4100/6393\n",
      "running for batch 4200/6393\n",
      "running for batch 4300/6393\n",
      "running for batch 4400/6393\n",
      "running for batch 4500/6393\n",
      "running for batch 4600/6393\n",
      "running for batch 4700/6393\n",
      "running for batch 4800/6393\n",
      "running for batch 4900/6393\n",
      "running for batch 5000/6393\n",
      "running for batch 5100/6393\n",
      "running for batch 5200/6393\n",
      "running for batch 5300/6393\n",
      "running for batch 5400/6393\n",
      "running for batch 5500/6393\n",
      "running for batch 5600/6393\n",
      "running for batch 5700/6393\n",
      "running for batch 5800/6393\n",
      "running for batch 5900/6393\n",
      "running for batch 6000/6393\n",
      "running for batch 6100/6393\n",
      "running for batch 6200/6393\n",
      "running for batch 6300/6393\n",
      "Training Loss: 56094.4534\n",
      "Dev Accuracy: 0.9404\n",
      "=====================\n",
      "starting epoch 3/5\n",
      "running for batch 0/6393\n",
      "running for batch 100/6393\n",
      "running for batch 200/6393\n",
      "running for batch 300/6393\n",
      "running for batch 400/6393\n",
      "running for batch 500/6393\n",
      "running for batch 600/6393\n",
      "running for batch 700/6393\n",
      "running for batch 800/6393\n",
      "running for batch 900/6393\n",
      "running for batch 1000/6393\n",
      "running for batch 1100/6393\n",
      "running for batch 1200/6393\n",
      "running for batch 1300/6393\n",
      "running for batch 1400/6393\n",
      "running for batch 1500/6393\n",
      "running for batch 1600/6393\n",
      "running for batch 1700/6393\n",
      "running for batch 1800/6393\n",
      "running for batch 1900/6393\n",
      "running for batch 2000/6393\n",
      "running for batch 2100/6393\n",
      "running for batch 2200/6393\n",
      "running for batch 2300/6393\n",
      "running for batch 2400/6393\n",
      "running for batch 2500/6393\n",
      "running for batch 2600/6393\n",
      "running for batch 2700/6393\n",
      "running for batch 2800/6393\n",
      "running for batch 2900/6393\n",
      "running for batch 3000/6393\n",
      "running for batch 3100/6393\n",
      "running for batch 3200/6393\n",
      "running for batch 3300/6393\n",
      "running for batch 3400/6393\n",
      "running for batch 3500/6393\n",
      "running for batch 3600/6393\n",
      "running for batch 3700/6393\n",
      "running for batch 3800/6393\n",
      "running for batch 3900/6393\n",
      "running for batch 4000/6393\n",
      "running for batch 4100/6393\n",
      "running for batch 4200/6393\n",
      "running for batch 4300/6393\n",
      "running for batch 4400/6393\n",
      "running for batch 4500/6393\n",
      "running for batch 4600/6393\n",
      "running for batch 4700/6393\n",
      "running for batch 4800/6393\n",
      "running for batch 4900/6393\n",
      "running for batch 5000/6393\n",
      "running for batch 5100/6393\n",
      "running for batch 5200/6393\n",
      "running for batch 5300/6393\n",
      "running for batch 5400/6393\n",
      "running for batch 5500/6393\n",
      "running for batch 5600/6393\n",
      "running for batch 5700/6393\n",
      "running for batch 5800/6393\n",
      "running for batch 5900/6393\n",
      "running for batch 6000/6393\n",
      "running for batch 6100/6393\n",
      "running for batch 6200/6393\n",
      "running for batch 6300/6393\n",
      "Training Loss: 56359.0098\n",
      "Dev Accuracy: 0.9404\n",
      "=====================\n",
      "starting epoch 4/5\n",
      "running for batch 0/6393\n",
      "running for batch 100/6393\n",
      "running for batch 200/6393\n",
      "running for batch 300/6393\n",
      "running for batch 400/6393\n",
      "running for batch 500/6393\n",
      "running for batch 600/6393\n",
      "running for batch 700/6393\n",
      "running for batch 800/6393\n",
      "running for batch 900/6393\n",
      "running for batch 1000/6393\n",
      "running for batch 1100/6393\n",
      "running for batch 1200/6393\n",
      "running for batch 1300/6393\n",
      "running for batch 1400/6393\n",
      "running for batch 1500/6393\n",
      "running for batch 1600/6393\n",
      "running for batch 1700/6393\n",
      "running for batch 1800/6393\n",
      "running for batch 1900/6393\n",
      "running for batch 2000/6393\n",
      "running for batch 2100/6393\n",
      "running for batch 2200/6393\n",
      "running for batch 2300/6393\n",
      "running for batch 2400/6393\n",
      "running for batch 2500/6393\n",
      "running for batch 2600/6393\n",
      "running for batch 2700/6393\n",
      "running for batch 2800/6393\n",
      "running for batch 2900/6393\n",
      "running for batch 3000/6393\n",
      "running for batch 3100/6393\n",
      "running for batch 3200/6393\n",
      "running for batch 3300/6393\n",
      "running for batch 3400/6393\n",
      "running for batch 3500/6393\n",
      "running for batch 3600/6393\n",
      "running for batch 3700/6393\n",
      "running for batch 3800/6393\n",
      "running for batch 3900/6393\n",
      "running for batch 4000/6393\n",
      "running for batch 4100/6393\n",
      "running for batch 4200/6393\n",
      "running for batch 4300/6393\n",
      "running for batch 4400/6393\n",
      "running for batch 4500/6393\n",
      "running for batch 4600/6393\n",
      "running for batch 4700/6393\n",
      "running for batch 4800/6393\n",
      "running for batch 4900/6393\n",
      "running for batch 5000/6393\n",
      "running for batch 5100/6393\n",
      "running for batch 5200/6393\n",
      "running for batch 5300/6393\n",
      "running for batch 5400/6393\n",
      "running for batch 5500/6393\n",
      "running for batch 5600/6393\n",
      "running for batch 5700/6393\n",
      "running for batch 5800/6393\n",
      "running for batch 5900/6393\n",
      "running for batch 6000/6393\n",
      "running for batch 6100/6393\n",
      "running for batch 6200/6393\n",
      "running for batch 6300/6393\n",
      "Training Loss: 56333.0406\n",
      "Dev Accuracy: 0.9404\n",
      "=====================\n",
      "starting epoch 5/5\n",
      "running for batch 0/6393\n",
      "running for batch 100/6393\n",
      "running for batch 200/6393\n",
      "running for batch 300/6393\n",
      "running for batch 400/6393\n",
      "running for batch 500/6393\n",
      "running for batch 600/6393\n",
      "running for batch 700/6393\n",
      "running for batch 800/6393\n",
      "running for batch 900/6393\n",
      "running for batch 1000/6393\n",
      "running for batch 1100/6393\n",
      "running for batch 1200/6393\n",
      "running for batch 1300/6393\n",
      "running for batch 1400/6393\n",
      "running for batch 1500/6393\n",
      "running for batch 1600/6393\n",
      "running for batch 1700/6393\n",
      "running for batch 1800/6393\n",
      "running for batch 1900/6393\n",
      "running for batch 2000/6393\n",
      "running for batch 2100/6393\n",
      "running for batch 2200/6393\n",
      "running for batch 2300/6393\n",
      "running for batch 2400/6393\n",
      "running for batch 2500/6393\n",
      "running for batch 2600/6393\n",
      "running for batch 2700/6393\n",
      "running for batch 2800/6393\n",
      "running for batch 2900/6393\n",
      "running for batch 3000/6393\n",
      "running for batch 3100/6393\n",
      "running for batch 3200/6393\n",
      "running for batch 3300/6393\n",
      "running for batch 3400/6393\n",
      "running for batch 3500/6393\n",
      "running for batch 3600/6393\n",
      "running for batch 3700/6393\n",
      "running for batch 3800/6393\n",
      "running for batch 3900/6393\n",
      "running for batch 4000/6393\n",
      "running for batch 4100/6393\n",
      "running for batch 4200/6393\n",
      "running for batch 4300/6393\n",
      "running for batch 4400/6393\n",
      "running for batch 4500/6393\n",
      "running for batch 4600/6393\n",
      "running for batch 4700/6393\n",
      "running for batch 4800/6393\n",
      "running for batch 4900/6393\n",
      "running for batch 5000/6393\n",
      "running for batch 5100/6393\n",
      "running for batch 5200/6393\n",
      "running for batch 5300/6393\n",
      "running for batch 5400/6393\n",
      "running for batch 5500/6393\n",
      "running for batch 5600/6393\n",
      "running for batch 5700/6393\n",
      "running for batch 5800/6393\n",
      "running for batch 5900/6393\n",
      "running for batch 6000/6393\n",
      "running for batch 6100/6393\n",
      "running for batch 6200/6393\n",
      "running for batch 6300/6393\n",
      "Training Loss: 46117.6345\n",
      "Dev Accuracy: 0.9396\n"
     ]
    }
   ],
   "source": [
    "# Seet seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# We get the model and label mapping from the training\n",
    "# Strange way to do it but whatever\n",
    "model, _, id2label = train_ClassModel(train_file_path = 'en_ewt-ud-train.iob2',\n",
    "                 dev_file_path = 'en_ewt-ud-dev.iob2',\n",
    "                 MLM = 'distilbert-base-cased',\n",
    "                 UNK = '[UNK]',\n",
    "                 batch_size = 32,\n",
    "                 lr = 0.0001,\n",
    "                 device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                 n_epochs = 5,\n",
    "                 max_train_sents = None,\n",
    "                 return_model = True,\n",
    "                 return_mappings = True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make Predictions on Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test data...\n",
      "Accuracy on test data: 0.9304\n"
     ]
    }
   ],
   "source": [
    "# Redefine variables\n",
    "tokzr = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "UNK = \"[UNK]\"\n",
    "batch_size = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PAD = tokzr.pad_token_id\n",
    "\n",
    "# Loading in test data\n",
    "test_sents, test_labels = read_universal_NER('en_ewt-ud-test.iob2')\n",
    "train_sents, train_labels = read_universal_NER('en_ewt-ud-train.iob2')\n",
    "id2label, label2id = myutils.labels2lookup(test_labels, UNK)\n",
    "test_labels = [label2id.get(label, label2id[UNK]) for label in test_labels]\n",
    "\n",
    "# Tokenize testing data\n",
    "test_tokked = myutils.tok(test_sents, tokzr)\n",
    "\n",
    "# Convert testing data to batches\n",
    "test_text_batched, test_labels_batched = myutils.to_batch(test_tokked, test_labels, batch_size, PAD, device)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "print('evaluating on test data...')\n",
    "test_score, test_predictions = model.run_eval(test_text_batched, test_labels_batched)\n",
    "print(f'Accuracy on test data: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Format Test Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that because of batching, there is a small amount of sentences\n",
    "### at the end of the test file which has not gotten predictions.\n",
    "### We can fix this issue later, but for now we will just ignore those last instances\n",
    "\n",
    "# temp_mapping = {0: '[UNK]', 1: 'O', 2: 'B-LOC', 3: 'I-LOC'}\n",
    "\n",
    "# Flatten batched predictions to non-batched list\n",
    "test_predictions_flat = [pred.item() for batch in test_predictions for pred in batch]\n",
    "\n",
    "# Convert to NER tags\n",
    "NER_predictions = [id2label[pred] for pred in test_predictions_flat]\n",
    "\n",
    "def insert_preds_to_file(masked_test_file_path, test_predictions_file_path, predictions):\n",
    "\n",
    "    # Open masked test file\n",
    "    with open(masked_test_file_path, 'r', encoding = 'utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    # Store output lines as batch index\n",
    "    output_lines = []\n",
    "    pred_idx = 0\n",
    "\n",
    "    # Iterate through each line in masked file and make copy to new file\n",
    "    for line in lines:\n",
    "\n",
    "        # Non-token lines\n",
    "        if line.startswith('#'):\n",
    "            output_lines.append(line)\n",
    "\n",
    "        # Token lines\n",
    "        elif line.strip():\n",
    "\n",
    "            # Split line into parts and insert prediction into third column\n",
    "            parts = line.strip().split('\\t')\n",
    "            parts[2] = predictions[pred_idx]\n",
    "\n",
    "            # Move to next prediction\n",
    "            pred_idx += 1\n",
    "            output_lines.append('\\t'.join(parts) + '\\n')\n",
    "\n",
    "            # Stop when we run out of predictions\n",
    "            if pred_idx >= len(predictions):\n",
    "                break\n",
    "        \n",
    "        # Empty seperator lines\n",
    "        else:\n",
    "            output_lines.append('\\n')\n",
    "\n",
    "    # Write output to new file\n",
    "    with open(test_predictions_file_path, 'w', encoding = 'utf-8') as outfile:\n",
    "        outfile.writelines(output_lines)\n",
    "\n",
    "# NER_predictions = convert_to_NER_tags(test_predictions, temp_mapping)\n",
    "insert_preds_to_file('en_ewt-ud-test-masked.iob2', 'test_predictions.iob2', NER_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Project proposal\n",
    "\n",
    "The written proposal should consist of maximum one page in [ACL-format](https://github.com/acl-org/acl-style-files) (The bibliography does not count for the word limit). In here, you should explain the last three points from the list above and place your project in a larger context (previous work).\n",
    "\n",
    "Make sure your proposal is:\n",
    "* Novel to some extent\n",
    "* Doable within the time-frame\n",
    "\n",
    "*hint* The [ACL Anthology](https://aclanthology.org/) contains almost all peer-reviewed NLP papers.\n",
    "\n",
    "**Deadline: 03-04 on LearnIt (14:00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final project\n",
    "The final project has a maximum size of 5 pages (excluding bibliography and appendix), using the [ACL style files](https://github.com/acl-org/acl-style-files)\n",
    "\n",
    "Besides the main paper (discussed in class), you have to include:\n",
    "* Group contributions. State who was responsible for which part of the project. Here you may state if there\n",
    "were any serious unequal workloads among group members. This should be put in the appendix.\n",
    "* A report on usage of chatbots. We follow: https://2023.aclweb.org/blog/ACL-2023-policy/\n",
    "   * Add a section in appendix if you made use of a chatbot (since we do not use a Responsible NLP Checklist)\n",
    "   * Include each stage on the ACL policy, and indicate to what extend you used a chatbot\n",
    "   * Use with care!, you are responsible for the project and plagiarism, correctness etc.\n",
    "\n",
    "You can also put additional results and details in the appendix. However, the paper itself should be standalone, and understandable without consulting the appendix.\n",
    "\n",
    "Furthermore, the code should be available on www.github.itu.dk (with a link in a footnote at the end of the abstract) , it should include a README with instructions on how to reproduce your results.\n",
    "\n",
    "**Deadline: 24-05 on LearnIt (14:00)** Please check the checklist below before uploading!\n",
    "\n",
    "Optionally, you can upload a draft a week before **17-05 (before 09:00)** for an extra round of feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Analysis is essential for the interpretation of your results. In this section we will shortly describe some different types of analysis. We strongly suggest to use at least one of these:\n",
    "\n",
    "* **Ablation study**: Leave out a certain part of the model, to study its effects. For example, disable the tokenizer, remove a certain (group of) feature(s), or disable the stop-word removal. If the performance drops a lot, it means that this part of the model contributes heavily to the models final performance. This is commonly done in 1 table, while disabling different parts of the model. Note that you can also do this the other way around, i.e. use only one feature (group) at a time, and test performance\n",
    "* **Learning curve**: Evaluate how much data your model needs to reach a certain performance. Especially for the data augmentation projects this is essential.\n",
    "* **Quantitative analysis**: Automated means of analyzing in which cases your model performs worse. This can for example be done with a confusion matrix.\n",
    "* **Qualitative analysis**: Manually inspect a certain number of errors, and try to categorize them/find trends. Can be combined with the quantitative analysis, i.e., inspect 100 cases of positive reviews predicted to be negative and 100 cases of negative reviews predicted to be positive\n",
    "* **Feature importance**: In traditional machine learning methods, one can often extract and inspect the weights of the features. In sklearn these can be found in: `trained_model.coef_`\n",
    "* **Other metrics**: per class scores, partial matches, or count how often the span-borders were correct, but the label wrong.\n",
    "* **Input words importance**: To gain insight into which words have a impact on prediction performance (positive, negative), we can analyze per-word impact: given a trained model, replace a given word with\n",
    "the unknown word token and observe the change in prediction score (probability for a class). This is\n",
    "shown in Figure 4 of [Rethmeier et al (2018)](https://aclweb.org/anthology/W18-6246) (a paper on controversy detection), also shown below: red-colored\n",
    "tokens were important for controversy detection, blue-colored token decreased prediction scores.\n",
    "\n",
    "<img width=400px src=example.png>\n",
    "\n",
    "Note that this is a non-exhaustive list, and you are encouraged to also explore additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist final project\n",
    "Please check all these items before handing in your final report. You only have to upload a pdf file on learnit, and make sure a link to the code is included in the report and the code is accesible. \n",
    "\n",
    "* Are all group members and their email addresses specified?\n",
    "* Does the group report include a representative project title?\n",
    "* Does the group report contain an abstract?\n",
    "* Does the introduction clearly specify the research intention and research question?\n",
    "* Does the group report adequately refer to the relevant literature?\n",
    "* Does the group report properly use figure, tables and examples?\n",
    "* Does the group report provide and discuss the empirical results?\n",
    "* Is the group report proofread?\n",
    "* Does the pdf contain the link to the project’s github repo?\n",
    "* Is the github repo accessible to the public (within ITU)?\n",
    "* Is the group report maximum 5 pages long, excluding references and appendix?\n",
    "* Are the group contributions added in the appendix?\n",
    "* Does the repository contain all scripts and code to reproduce the results in the group report? Are instructions\n",
    " provided on how to run the code?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "56941cacf15e8b05765996006082865469347c2b4cdce983108d1335de8b4245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
